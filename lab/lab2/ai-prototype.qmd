---
title: "Lab 2 AI Prototype"
subtitle: "Context Engineering"
---

## Learning Goals

The goal of this prototype is to experiment with using AI as a coding assistant to generate HDL, and to validate whether . By the end of this experiment you should be able to:

- Attach a file as context to a prompt for a LLM agent.  
- Describe the effect that context has on a prompt.

## Prototype

Fire up your favorite LLM. (ChatGPT, Claude, Copilot, Gemini, etc.). Enter the prompt below.

::: {.callout-note title="LLM Prompt"}
Write SystemVerilog HDL to time multiplex a single seven segment decoder (that decodes from four bits to a common anode seven segment display) to decode two sets of input bits and drive two sets of seven output bits.
:::

Create a new Radiant project, type the code generated by the LLM in and analyze the results. 

Now start a new AI conversation (don't leave the old one in your history), attach your lab 1 files as context (usually using a + or attach symbol in the chat box) and write the following, similar prompt:

::: {.callout-note title="LLM Prompt"}
Write SystemVerilog HDL to time multiplex a single seven segment decoder (that decodes from four bits to a common anode seven segment display) to decode two sets of input bits and drive two sets of seven output bits.  Use the seven segment decoder and oscillator provided in the attached files.
:::

Create a new Radiant project, type the code generated by the LLM in and analyze the results. 

## Reflect

Write up a few paragraphs reflecting on your experience using the LLM to help you code. Feel free to make full use of screenshots, code snippets, and other media as you write your reflections.

Here are a few ideas of on what you might comment on:

- How would you rate the quality of the output and why?
- What SystemVerilog constructs/syntax did the LLM generated that were new to you?
- Did the LLM-generated code synthesize the first time around? If not, what were the issues?
- What error or warning messages did Radiant output?
- What would you do differently the next time you use an LLM in your workflow?

